'use client';

import { useState, useEffect } from 'react';
import { config } from '@/lib/config';

export interface AIModel {
  id: string; // æ¨¡å‹åç§°ï¼ˆå¦‚ grok, claude, geminiï¼‰
  dbId: string; // æ•°æ®åº“ ID
  name: string; // æ˜¾ç¤ºåç§°
  provider: string; // æä¾›å•†
  modelId: string; // å®é™…æ¨¡å‹ ID
  icon: string; // emoji æˆ–å›¾æ ‡è·¯å¾„
  iconUrl: string; // å›¾æ ‡ URL
  color: string; // Tailwind é¢œè‰²ç±»
  description: string; // æè¿°
  isDefault: boolean; // æ˜¯å¦é»˜è®¤
}

// ç¼“å­˜æ¨¡å‹åˆ—è¡¨ï¼ˆé¿å…é‡å¤è¯·æ±‚ï¼‰
let cachedModels: AIModel[] | null = null;
let cacheTimestamp: number = 0;
const CACHE_DURATION = 5 * 60 * 1000; // 5åˆ†é’Ÿç¼“å­˜

/**
 * è·å–å·²å¯ç”¨çš„ AI æ¨¡å‹åˆ—è¡¨ Hook
 */
export function useAIModels() {
  const [models, setModels] = useState<AIModel[]>(cachedModels || []);
  const [loading, setLoading] = useState(!cachedModels);
  const [error, setError] = useState<string | null>(null);

  useEffect(() => {
    const fetchModels = async () => {
      // æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
      const now = Date.now();
      if (cachedModels && now - cacheTimestamp < CACHE_DURATION) {
        setModels(cachedModels);
        setLoading(false);
        return;
      }

      try {
        setLoading(true);
        const response = await fetch(`${config.apiUrl}/ai/models`);
        if (response.ok) {
          const data = await response.json();
          cachedModels = data;
          cacheTimestamp = now;
          setModels(data);
          setError(null);
        } else {
          throw new Error('Failed to fetch AI models');
        }
      } catch (err) {
        console.error('Failed to fetch AI models:', err);
        setError(err instanceof Error ? err.message : 'Unknown error');
        // å¦‚æœè·å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤çš„ç¡¬ç¼–ç åˆ—è¡¨ä½œä¸ºåå¤‡
        setModels(getDefaultModels());
      } finally {
        setLoading(false);
      }
    };

    fetchModels();
  }, []);

  return { models, loading, error };
}

/**
 * æ¸…é™¤æ¨¡å‹ç¼“å­˜ï¼ˆå½“ç®¡ç†å‘˜ä¿®æ”¹æ¨¡å‹é…ç½®åè°ƒç”¨ï¼‰
 */
export function clearAIModelsCache() {
  cachedModels = null;
  cacheTimestamp = 0;
}

/**
 * é»˜è®¤æ¨¡å‹åˆ—è¡¨ï¼ˆåå¤‡æ–¹æ¡ˆï¼‰
 */
function getDefaultModels(): AIModel[] {
  return [
    {
      id: 'grok',
      dbId: '',
      name: 'Grok (xAI)',
      provider: 'xAI',
      modelId: 'grok-3-latest',
      icon: 'ğŸ¤–',
      iconUrl: '/icons/ai/grok.svg',
      color: 'from-blue-500 to-blue-600',
      description: 'xAI Grok - å¿«é€Ÿæ™ºèƒ½',
      isDefault: true,
    },
    {
      id: 'gpt-4',
      dbId: '',
      name: 'ChatGPT (OpenAI)',
      provider: 'OpenAI',
      modelId: 'gpt-4-turbo',
      icon: 'ğŸ§ ',
      iconUrl: '/icons/ai/openai.svg',
      color: 'from-green-500 to-green-600',
      description: 'OpenAI ChatGPT - æ·±åº¦æ€è€ƒ',
      isDefault: false,
    },
    {
      id: 'claude',
      dbId: '',
      name: 'Claude (Anthropic)',
      provider: 'Anthropic',
      modelId: 'claude-sonnet-4-20250514',
      icon: 'ğŸ­',
      iconUrl: '/icons/ai/claude.svg',
      color: 'from-orange-500 to-orange-600',
      description: 'Anthropic Claude - å¯¹è¯ä¸“å®¶',
      isDefault: false,
    },
    {
      id: 'gemini',
      dbId: '',
      name: 'Gemini (Google)',
      provider: 'Google',
      modelId: 'gemini-2.0-flash',
      icon: 'ğŸ’',
      iconUrl: '/icons/ai/gemini.svg',
      color: 'from-purple-500 to-purple-600',
      description: 'Google Gemini - å¤šæ¨¡æ€',
      isDefault: false,
    },
  ];
}
